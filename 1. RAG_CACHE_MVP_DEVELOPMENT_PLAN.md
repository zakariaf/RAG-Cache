# RAG Cache MVP - Complete Development Plan
## Tech Co-Founder Specification Document

**Project:** RAGBoost - Token-Efficient RAG Caching Platform  
**Architecture:** Python (FastAPI) + Rails API Consumer  
**Philosophy:** Sandi Metz POOD Principles - Small Classes, Clear Names, Single Responsibility  
**Timeline:** 3 weeks MVP → 6 weeks production-ready  

---

## Table of Contents
1. [System Architecture](#system-architecture)
2. [Tech Stack](#tech-stack)
3. [Project Structure](#project-structure)
4. [Core Principles](#core-principles)
5. [Development Phases](#development-phases)
6. [Complete Task List (200+ Issues)](#complete-task-list)
7. [Sample Code](#sample-code)
8. [Docker Configuration](#docker-configuration)
9. [Testing Strategy](#testing-strategy)
10. [API Specifications](#api-specifications)

---

## System Architecture

### High-Level Flow
```
User Request → Rails API → Python FastAPI → Cache Check (Redis) 
                                          ↓
                                    Cache Miss?
                                          ↓
                              Semantic Search (Qdrant)
                                          ↓
                              Similar Query Found?
                                          ↓
                                   Yes → Return Cached
                                   No → LLM Layer → Cache Result → Return
```

### Service Boundaries

**Python Service (Port 8000):**
- REST API endpoint `/api/v1/query`
- Redis cache operations
- Qdrant vector operations
- LLM abstraction layer
- Query embedding generation
- Semantic similarity matching
- Usage metrics collection

**Rails Service (Port 3000):**
- User authentication (Devise)
- API key management
- Billing (Stripe)
- Usage dashboard
- Analytics
- Calls Python service via HTTP

### Data Flow

1. **Rails** validates API key → forwards to Python
2. **Python** receives query with metadata
3. **Cache Layer** checks exact match (Redis)
4. **Semantic Layer** checks similar queries (Qdrant)
5. **LLM Layer** calls provider if cache miss
6. **Response** cached and returned
7. **Metrics** sent back to Rails for billing

---

## Tech Stack

### Python Service
```yaml
Framework: FastAPI 0.104.1
Language: Python 3.11
Web Server: Uvicorn
Cache: Redis 7.2
Vector DB: Qdrant 1.6
LLM Clients: openai, anthropic
Testing: pytest, pytest-asyncio, pytest-cov
Code Quality: black, flake8, mypy, isort
Validation: pydantic
HTTP Client: httpx (async)
Environment: python-dotenv
```

### Infrastructure
```yaml
Containers: Docker, docker-compose
Orchestration: docker-compose.yml
Networks: Internal bridge network
Volumes: Persistent for Redis, Qdrant, Postgres
```

### Development Tools
```yaml
Pre-commit hooks: black, flake8, mypy
CI/CD: GitHub Actions
Monitoring: Prometheus metrics endpoint
Logging: structlog
Documentation: OpenAPI (FastAPI auto-gen)
```

---

## Project Structure

```
ragcache-python/
├── docker-compose.yml
├── Dockerfile
├── requirements.txt
├── requirements-dev.txt
├── pytest.ini
├── .env.example
├── .dockerignore
├── .gitignore
├── README.md
│
├── app/
│   ├── __init__.py
│   ├── main.py                    # FastAPI application entry
│   ├── config.py                  # Configuration management
│   │
│   ├── api/
│   │   ├── __init__.py
│   │   ├── deps.py                # Dependency injection
│   │   ├── routes/
│   │   │   ├── __init__.py
│   │   │   ├── health.py          # Health check endpoint
│   │   │   └── query.py           # Main query endpoint
│   │   └── middleware/
│   │       ├── __init__.py
│   │       ├── auth.py            # API key validation
│   │       ├── logging.py         # Request logging
│   │       └── metrics.py         # Prometheus metrics
│   │
│   ├── cache/
│   │   ├── __init__.py
│   │   ├── manager.py             # Cache orchestrator
│   │   ├── redis_cache.py         # Redis operations
│   │   ├── semantic_cache.py      # Qdrant operations
│   │   └── cache_key.py           # Key generation
│   │
│   ├── llm/
│   │   ├── __init__.py
│   │   ├── provider.py            # LLM abstraction layer
│   │   ├── openai_provider.py     # OpenAI implementation
│   │   ├── anthropic_provider.py  # Anthropic implementation
│   │   ├── factory.py             # Provider factory
│   │   └── models.py              # Request/response models
│   │
│   ├── embeddings/
│   │   ├── __init__.py
│   │   ├── generator.py           # Embedding generator
│   │   └── normalizer.py          # Vector normalization
│   │
│   ├── similarity/
│   │   ├── __init__.py
│   │   ├── matcher.py             # Semantic matching
│   │   └── scorer.py              # Similarity scoring
│   │
│   ├── models/
│   │   ├── __init__.py
│   │   ├── query.py               # Query models
│   │   ├── response.py            # Response models
│   │   ├── cache_entry.py         # Cache entry models
│   │   └── metrics.py             # Metrics models
│   │
│   ├── repositories/
│   │   ├── __init__.py
│   │   ├── redis_repository.py    # Redis data access
│   │   └── qdrant_repository.py   # Qdrant data access
│   │
│   ├── services/
│   │   ├── __init__.py
│   │   ├── query_service.py       # Main query orchestration
│   │   ├── cache_service.py       # Cache operations
│   │   └── metrics_service.py     # Metrics collection
│   │
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── logger.py              # Structured logging
│   │   ├── hasher.py              # Hash generation
│   │   └── validators.py          # Input validation
│   │
│   └── exceptions/
│       ├── __init__.py
│       ├── cache_exceptions.py
│       ├── llm_exceptions.py
│       └── validation_exceptions.py
│
├── tests/
│   ├── __init__.py
│   ├── conftest.py                # Pytest fixtures
│   │
│   ├── unit/
│   │   ├── __init__.py
│   │   ├── cache/
│   │   │   ├── test_redis_cache.py
│   │   │   ├── test_semantic_cache.py
│   │   │   └── test_cache_manager.py
│   │   ├── llm/
│   │   │   ├── test_provider.py
│   │   │   ├── test_openai_provider.py
│   │   │   └── test_factory.py
│   │   ├── embeddings/
│   │   │   └── test_generator.py
│   │   └── similarity/
│   │       └── test_matcher.py
│   │
│   ├── integration/
│   │   ├── __init__.py
│   │   ├── test_query_flow.py
│   │   ├── test_cache_flow.py
│   │   └── test_api_endpoints.py
│   │
│   └── mocks/
│       ├── __init__.py
│       ├── mock_llm.py
│       └── mock_redis.py
│
├── scripts/
│   ├── setup_dev.sh               # Development setup
│   ├── run_tests.sh               # Test runner
│   └── seed_data.py               # Sample data seeding
│
└── docs/
    ├── API.md                     # API documentation
    ├── ARCHITECTURE.md            # Architecture details
    └── DEPLOYMENT.md              # Deployment guide
```

---

## Core Principles (Sandi Metz Rules)

### 1. Class Size
- **Max 100 lines per class**
- **Single Responsibility** - Each class does ONE thing
- **Clear naming** - Name describes exact purpose

### 2. Method Size
- **Max 5 lines per method** (ideal)
- **Max 10 lines** (absolute limit)
- **Extract to private methods** if longer

### 3. Method Arguments
- **Max 4 parameters** per method
- **Use objects** to group related params
- **Keyword-only arguments** in Python

### 4. Naming Conventions
```python
# Classes: NounPhrase, PascalCase
class CacheManager:
class RedisRepository:

# Methods: VerbPhrase, snake_case
def fetch_from_cache():
def store_query_result():

# Variables: Descriptive, snake_case
cache_key = "query:hash:123"
similarity_threshold = 0.85
```

### 5. Dependency Injection
```python
# Good - Dependencies injected
class QueryService:
    def __init__(
        self,
        cache_manager: CacheManager,
        llm_provider: LLMProvider
    ):
        self._cache = cache_manager
        self._llm = llm_provider

# Bad - Hard-coded dependencies
class QueryService:
    def __init__(self):
        self._cache = CacheManager()  # ❌ Hard-coded
```

### 6. Testing Philosophy
- **Test behavior, not implementation**
- **One assertion per test** (when possible)
- **Clear test names** - test_should_xxx_when_yyy

---

## Development Phases

### Phase 1: Foundation (Week 1, Days 1-7)
**Goal:** Working skeleton with Docker, basic API, core infrastructure

**Deliverables:**
- Docker setup with all services
- FastAPI skeleton with health check
- Redis + Qdrant connected
- Basic API endpoint structure
- CI/CD pipeline

### Phase 2: Core Logic (Week 2, Days 8-14)
**Goal:** Caching logic, LLM layer, semantic matching

**Deliverables:**
- Cache manager with Redis
- Semantic cache with Qdrant
- LLM abstraction layer
- OpenAI + Anthropic providers
- Query processing pipeline

### Phase 3: Production Ready (Week 3, Days 15-21)
**Goal:** Testing, monitoring, docs, optimization

**Deliverables:**
- 80%+ test coverage
- Metrics and logging
- API documentation
- Performance optimization
- Rails integration guide

---

## Complete Task List (200+ GitHub Issues)

### Epic 1: Project Setup & Infrastructure (Issues #1-25)

#### Issue #1: Initialize Python Project
**Labels:** `setup`, `infrastructure`
**Priority:** P0
**Estimate:** 30 minutes

**Description:**
Create base Python project structure with modern tooling.

**Tasks:**
- [ ] Create project directory: `ragcache-python`
- [ ] Initialize git repository
- [ ] Create `.gitignore` for Python
- [ ] Create README.md with project description
- [ ] Set up virtual environment structure

**Acceptance Criteria:**
- Project directory exists with git initialized
- README includes project description
- .gitignore excludes venv, __pycache__, .env

**Sample Commands:**
```bash
mkdir ragcache-python && cd ragcache-python
git init
python -m venv venv
```

---

#### Issue #2: Create Requirements Files
**Labels:** `setup`, `dependencies`
**Priority:** P0
**Estimate:** 1 hour

**Description:**
Define all Python dependencies with version pinning.

**Tasks:**
- [ ] Create `requirements.txt` for production
- [ ] Create `requirements-dev.txt` for development
- [ ] Pin all versions for reproducibility
- [ ] Document why each dependency is needed

**requirements.txt:**
```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
redis==5.0.1
qdrant-client==1.6.4
openai==1.3.5
anthropic==0.7.2
httpx==0.25.1
python-dotenv==1.0.0
structlog==23.2.0
prometheus-client==0.19.0
```

**requirements-dev.txt:**
```txt
-r requirements.txt
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
pytest-mock==3.12.0
black==23.11.0
flake8==6.1.0
mypy==1.7.1
isort==5.12.0
httpx-mock==0.11.0
```

**Acceptance Criteria:**
- All dependencies listed with versions
- Separate files for prod and dev
- Can install with `pip install -r requirements.txt`

---

#### Issue #3: Docker Compose Configuration
**Labels:** `infrastructure`, `docker`
**Priority:** P0
**Estimate:** 2 hours

**Description:**
Set up docker-compose.yml with all services (Python, Redis, Qdrant).

**Tasks:**
- [ ] Create `docker-compose.yml`
- [ ] Configure Python service
- [ ] Configure Redis service
- [ ] Configure Qdrant service
- [ ] Set up networks
- [ ] Configure volumes for persistence
- [ ] Add health checks

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ragcache-api
    ports:
      - "8000:8000"
    env_file:
      - .env
    depends_on:
      - redis
      - qdrant
    networks:
      - ragcache-network
    volumes:
      - ./app:/app/app
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7.2-alpine
    container_name: ragcache-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - ragcache-network
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  qdrant:
    image: qdrant/qdrant:v1.6.1
    container_name: ragcache-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
    networks:
      - ragcache-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  ragcache-network:
    driver: bridge

volumes:
  redis-data:
  qdrant-data:
```

**Acceptance Criteria:**
- `docker-compose up` starts all services
- All services pass health checks
- Services can communicate on internal network
- Data persists across restarts

---

#### Issue #4: Create Dockerfile
**Labels:** `infrastructure`, `docker`
**Priority:** P0
**Estimate:** 1 hour

**Description:**
Multi-stage Dockerfile for Python application.

**Dockerfile:**
```dockerfile
# Stage 1: Base
FROM python:3.11-slim as base

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Stage 2: Dependencies
FROM base as dependencies

COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Stage 3: Application
FROM dependencies as application

COPY ./app ./app

# Create non-root user
RUN useradd -m -u 1000 appuser && \
    chown -R appuser:appuser /app

USER appuser

EXPOSE 8000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**.dockerignore:**
```
__pycache__
*.pyc
*.pyo
*.pyd
.Python
venv/
.env
.git
.gitignore
.pytest_cache
.coverage
htmlcov/
dist/
build/
*.egg-info
.mypy_cache
.vscode
.idea
```

**Acceptance Criteria:**
- Image builds successfully
- Non-root user runs application
- Image size optimized
- No sensitive files included

---

#### Issue #5: Environment Configuration
**Labels:** `setup`, `configuration`
**Priority:** P0
**Estimate:** 30 minutes

**Description:**
Environment variable management with example file.

**.env.example:**
```bash
# Application
APP_NAME=RAGCache
APP_ENV=development
LOG_LEVEL=INFO

# API
API_HOST=0.0.0.0
API_PORT=8000

# Redis
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=

# Qdrant
QDRANT_HOST=qdrant
QDRANT_PORT=6333
QDRANT_COLLECTION_NAME=query_embeddings

# LLM Providers
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...

# Default LLM Settings
DEFAULT_LLM_PROVIDER=openai
DEFAULT_MODEL=gpt-3.5-turbo
DEFAULT_MAX_TOKENS=1000
DEFAULT_TEMPERATURE=0.7

# Cache Settings
CACHE_TTL_SECONDS=3600
SEMANTIC_SIMILARITY_THRESHOLD=0.85
MAX_CACHE_SIZE_MB=1000

# Metrics
ENABLE_METRICS=true
METRICS_PORT=9090
```

**Tasks:**
- [ ] Create `.env.example`
- [ ] Add `.env` to `.gitignore`
- [ ] Document all variables
- [ ] Create config loader

**Acceptance Criteria:**
- Example file has all required variables
- .env excluded from git
- Clear documentation for each variable

---

#### Issue #6: Create Config Module
**Labels:** `core`, `configuration`
**Priority:** P0
**Estimate:** 1 hour

**Description:**
Centralized configuration management using Pydantic Settings.

**app/config.py:**
```python
from typing import Literal
from pydantic_settings import BaseSettings, SettingsConfigDict


class AppConfig(BaseSettings):
    """Application configuration."""
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False
    )
    
    # Application
    app_name: str = "RAGCache"
    app_env: Literal["development", "production", "test"] = "development"
    log_level: str = "INFO"
    
    # API
    api_host: str = "0.0.0.0"
    api_port: int = 8000
    
    # Redis
    redis_host: str = "localhost"
    redis_port: int = 6379
    redis_db: int = 0
    redis_password: str = ""
    
    # Qdrant
    qdrant_host: str = "localhost"
    qdrant_port: int = 6333
    qdrant_collection_name: str = "query_embeddings"
    
    # LLM Providers
    openai_api_key: str
    anthropic_api_key: str = ""
    
    # Default LLM Settings
    default_llm_provider: Literal["openai", "anthropic"] = "openai"
    default_model: str = "gpt-3.5-turbo"
    default_max_tokens: int = 1000
    default_temperature: float = 0.7
    
    # Cache Settings
    cache_ttl_seconds: int = 3600
    semantic_similarity_threshold: float = 0.85
    max_cache_size_mb: int = 1000
    
    # Metrics
    enable_metrics: bool = True
    metrics_port: int = 9090
    
    @property
    def redis_url(self) -> str:
        """Build Redis connection URL."""
        if self.redis_password:
            return f"redis://:{self.redis_password}@{self.redis_host}:{self.redis_port}/{self.redis_db}"
        return f"redis://{self.redis_host}:{self.redis_port}/{self.redis_db}"
    
    @property
    def qdrant_url(self) -> str:
        """Build Qdrant connection URL."""
        return f"http://{self.qdrant_host}:{self.qdrant_port}"
    
    @property
    def is_development(self) -> bool:
        """Check if running in development."""
        return self.app_env == "development"
    
    @property
    def is_production(self) -> bool:
        """Check if running in production."""
        return self.app_env == "production"


# Global config instance
config = AppConfig()
```

**Tasks:**
- [ ] Create config.py with Pydantic Settings
- [ ] Add all environment variables
- [ ] Add helper properties for URLs
- [ ] Add environment check helpers
- [ ] Write unit tests

**Acceptance Criteria:**
- Config loads from .env file
- All settings validated by Pydantic
- Helper methods work correctly
- Type hints present

---

#### Issue #7: Setup Structured Logging
**Labels:** `core`, `observability`
**Priority:** P0
**Estimate:** 1 hour

**Description:**
Structured logging with structlog for better observability.

**app/utils/logger.py:**
```python
import logging
import sys
from typing import Any

import structlog
from structlog.types import Processor

from app.config import config


def setup_logging() -> None:
    """Configure structured logging."""
    
    shared_processors: list[Processor] = [
        structlog.contextvars.merge_contextvars,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.UnicodeDecoder(),
    ]
    
    if config.is_development:
        # Pretty console output for development
        structlog.configure(
            processors=shared_processors + [
                structlog.dev.ConsoleRenderer()
            ],
            wrapper_class=structlog.stdlib.BoundLogger,
            context_class=dict,
            logger_factory=structlog.stdlib.LoggerFactory(),
            cache_logger_on_first_use=True,
        )
    else:
        # JSON output for production
        structlog.configure(
            processors=shared_processors + [
                structlog.processors.dict_tracebacks,
                structlog.processors.JSONRenderer()
            ],
            wrapper_class=structlog.stdlib.BoundLogger,
            context_class=dict,
            logger_factory=structlog.stdlib.LoggerFactory(),
            cache_logger_on_first_use=True,
        )
    
    # Set root logger level
    logging.basicConfig(
        format="%(message)s",
        stream=sys.stdout,
        level=getattr(logging, config.log_level.upper()),
    )


def get_logger(name: str) -> structlog.stdlib.BoundLogger:
    """Get a structured logger instance."""
    return structlog.get_logger(name)
```

**Tasks:**
- [ ] Create logger.py with structlog
- [ ] Configure development vs production formats
- [ ] Add helper function to get logger
- [ ] Test logging output

**Acceptance Criteria:**
- Structured logging works
- Different formats for dev/prod
- Easy to get logger in any module

---

#### Issue #8: FastAPI Application Skeleton
**Labels:** `api`, `core`
**Priority:** P0
**Estimate:** 1 hour

**Description:**
Create main FastAPI application with basic configuration.

**app/main.py:**
```python
from contextlib import asynccontextmanager
from typing import AsyncGenerator

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from app.api.routes import health, query
from app.config import config
from app.utils.logger import get_logger, setup_logging

# Setup logging
setup_logging()
logger = get_logger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """Application lifespan handler."""
    logger.info("Starting RAGCache application", env=config.app_env)
    
    # Startup: Initialize connections, etc.
    # TODO: Initialize Redis connection pool
    # TODO: Initialize Qdrant client
    
    yield
    
    # Shutdown: Close connections, cleanup
    logger.info("Shutting down RAGCache application")
    # TODO: Close Redis connection pool
    # TODO: Close Qdrant client


def create_application() -> FastAPI:
    """Create and configure FastAPI application."""
    
    app = FastAPI(
        title=config.app_name,
        description="Token-efficient RAG caching platform",
        version="0.1.0",
        docs_url="/docs" if config.is_development else None,
        redoc_url="/redoc" if config.is_development else None,
        lifespan=lifespan,
    )
    
    # Add CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # TODO: Restrict in production
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Include routers
    app.include_router(health.router, tags=["health"])
    app.include_router(query.router, prefix="/api/v1", tags=["query"])
    
    return app


app = create_application()


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "app.main:app",
        host=config.api_host,
        port=config.api_port,
        reload=config.is_development,
    )
```

**Tasks:**
- [ ] Create main.py with FastAPI app
- [ ] Add lifespan handler
- [ ] Configure CORS
- [ ] Include basic routers
- [ ] Add development server runner

**Acceptance Criteria:**
- FastAPI app starts successfully
- Docs available at /docs in development
- CORS configured
- Lifespan events trigger

---

#### Issue #9: Health Check Endpoint
**Labels:** `api`, `monitoring`
**Priority:** P0
**Estimate:** 30 minutes

**Description:**
Health check endpoint for Docker and monitoring.

**app/api/routes/health.py:**
```python
from fastapi import APIRouter
from pydantic import BaseModel

from app.config import config

router = APIRouter()


class HealthResponse(BaseModel):
    """Health check response."""
    
    status: str
    environment: str
    version: str


@router.get("/health", response_model=HealthResponse)
async def health_check() -> HealthResponse:
    """
    Health check endpoint.
    
    Returns application status and basic info.
    """
    return HealthResponse(
        status="healthy",
        environment=config.app_env,
        version="0.1.0",
    )


@router.get("/ready")
async def readiness_check() -> dict[str, str]:
    """
    Readiness check endpoint.
    
    Checks if application is ready to serve traffic.
    """
    # TODO: Check Redis connection
    # TODO: Check Qdrant connection
    
    return {"status": "ready"}
```

**Tasks:**
- [ ] Create health.py router
- [ ] Add /health endpoint
- [ ] Add /ready endpoint
- [ ] Add response models
- [ ] TODO: Add actual dependency checks

**Acceptance Criteria:**
- GET /health returns 200
- Response includes status and env
- /ready endpoint exists

---

#### Issue #10: Pytest Configuration
**Labels:** `testing`, `setup`
**Priority:** P0
**Estimate:** 1 hour

**Description:**
Configure pytest with async support and coverage.

**pytest.ini:**
```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
asyncio_mode = auto

# Coverage
addopts = 
    --cov=app
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=80
    -v
    --strict-markers
    --disable-warnings

markers =
    unit: Unit tests
    integration: Integration tests
    slow: Slow running tests
    redis: Tests requiring Redis
    qdrant: Tests requiring Qdrant
```

**tests/conftest.py:**
```python
import pytest
from fastapi.testclient import TestClient
from unittest.mock import MagicMock

from app.main import create_application


@pytest.fixture
def app():
    """Create FastAPI test application."""
    return create_application()


@pytest.fixture
def client(app):
    """Create test client."""
    return TestClient(app)


@pytest.fixture
def mock_redis():
    """Mock Redis client."""
    return MagicMock()


@pytest.fixture
def mock_qdrant():
    """Mock Qdrant client."""
    return MagicMock()


@pytest.fixture
def mock_openai():
    """Mock OpenAI client."""
    return MagicMock()
```

**Tasks:**
- [ ] Create pytest.ini
- [ ] Create conftest.py with fixtures
- [ ] Add test markers
- [ ] Configure coverage
- [ ] Write sample test

**Acceptance Criteria:**
- pytest runs successfully
- Coverage reports generated
- Fixtures work correctly

---

### Epic 2: Models & Data Structures (Issues #11-25)

#### Issue #11: Query Request Model
**Labels:** `models`, `api`
**Priority:** P0
**Estimate:** 30 minutes

**app/models/query.py:**
```python
from typing import Literal, Optional
from pydantic import BaseModel, Field, validator


class QueryRequest(BaseModel):
    """Incoming query request."""
    
    query: str = Field(
        ...,
        min_length=1,
        max_length=10000,
        description="User query text"
    )
    
    provider: Optional[Literal["openai", "anthropic"]] = Field(
        None,
        description="LLM provider to use"
    )
    
    model: Optional[str] = Field(
        None,
        description="Specific model to use"
    )
    
    max_tokens: Optional[int] = Field(
        None,
        ge=1,
        le=4000,
        description="Maximum tokens in response"
    )
    
    temperature: Optional[float] = Field(
        None,
        ge=0.0,
        le=2.0,
        description="Temperature for generation"
    )
    
    use_cache: bool = Field(
        True,
        description="Whether to use caching"
    )
    
    api_key: str = Field(
        ...,
        description="API key for authentication"
    )
    
    @validator("query")
    def validate_query(cls, v: str) -> str:
        """Validate query is not empty after stripping."""
        if not v.strip():
            raise ValueError("Query cannot be empty")
        return v.strip()
    
    class Config:
        json_schema_extra = {
            "example": {
                "query": "What is the capital of France?",
                "provider": "openai",
                "model": "gpt-3.5-turbo",
                "max_tokens": 100,
                "temperature": 0.7,
                "use_cache": True,
                "api_key": "rg_test_key_123"
            }
        }
```

**Acceptance Criteria:**
- Model validates all fields
- Example included
- Validators work correctly

---

#### Issue #12: Query Response Model
**Labels:** `models`, `api`
**Priority:** P0
**Estimate:** 30 minutes

**app/models/response.py:**
```python
from typing import Literal, Optional
from pydantic import BaseModel, Field


class UsageMetrics(BaseModel):
    """Token usage metrics."""
    
    prompt_tokens: int = Field(..., description="Tokens in prompt")
    completion_tokens: int = Field(..., description="Tokens in completion")
    total_tokens: int = Field(..., description="Total tokens used")


class CacheInfo(BaseModel):
    """Cache hit information."""
    
    cache_hit: bool = Field(..., description="Whether cache was hit")
    cache_type: Optional[Literal["exact", "semantic"]] = Field(
        None,
        description="Type of cache hit"
    )
    similarity_score: Optional[float] = Field(
        None,
        description="Similarity score for semantic match"
    )


class QueryResponse(BaseModel):
    """Query response."""
    
    response: str = Field(..., description="Generated response")
    
    provider: str = Field(..., description="LLM provider used")
    
    model: str = Field(..., description="Model used")
    
    usage: UsageMetrics = Field(..., description="Token usage")
    
    cache_info: CacheInfo = Field(..., description="Cache information")
    
    latency_ms: float = Field(..., description="Response latency in milliseconds")
    
    class Config:
        json_schema_extra = {
            "example": {
                "response": "The capital of France is Paris.",
                "provider": "openai",
                "model": "gpt-3.5-turbo",
                "usage": {
                    "prompt_tokens": 10,
                    "completion_tokens": 8,
                    "total_tokens": 18
                },
                "cache_info": {
                    "cache_hit": True,
                    "cache_type": "exact",
                    "similarity_score": None
                },
                "latency_ms": 45.2
            }
        }
```

---

#### Issue #13: Cache Entry Model
**Labels:** `models`, `cache`
**Priority:** P0
**Estimate:** 30 minutes

**app/models/cache_entry.py:**
```python
from datetime import datetime
from typing import Optional
from pydantic import BaseModel, Field


class CacheEntry(BaseModel):
    """Cached query-response pair."""
    
    query_hash: str = Field(..., description="Hash of normalized query")
    
    original_query: str = Field(..., description="Original query text")
    
    response: str = Field(..., description="Cached response")
    
    provider: str = Field(..., description="Provider used")
    
    model: str = Field(..., description="Model used")
    
    prompt_tokens: int = Field(..., description="Tokens in prompt")
    
    completion_tokens: int = Field(..., description="Tokens in response")
    
    embedding: Optional[list[float]] = Field(
        None,
        description="Query embedding vector"
    )
    
    created_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="When entry was created"
    )
    
    accessed_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="Last access time"
    )
    
    access_count: int = Field(
        default=1,
        description="Number of times accessed"
    )
    
    ttl_seconds: Optional[int] = Field(
        None,
        description="Time to live in seconds"
    )
    
    def to_redis_dict(self) -> dict:
        """Convert to Redis-compatible dict."""
        return {
            "query_hash": self.query_hash,
            "original_query": self.original_query,
            "response": self.response,
            "provider": self.provider,
            "model": self.model,
            "prompt_tokens": str(self.prompt_tokens),
            "completion_tokens": str(self.completion_tokens),
            "created_at": self.created_at.isoformat(),
            "accessed_at": self.accessed_at.isoformat(),
            "access_count": str(self.access_count),
        }
    
    @classmethod
    def from_redis_dict(cls, data: dict) -> "CacheEntry":
        """Create from Redis dict."""
        return cls(
            query_hash=data["query_hash"],
            original_query=data["original_query"],
            response=data["response"],
            provider=data["provider"],
            model=data["model"],
            prompt_tokens=int(data["prompt_tokens"]),
            completion_tokens=int(data["completion_tokens"]),
            created_at=datetime.fromisoformat(data["created_at"]),
            accessed_at=datetime.fromisoformat(data["accessed_at"]),
            access_count=int(data["access_count"]),
        )
```

---

### Issue #14-25: Continue with similar detail...

I'll continue this pattern for ALL 200+ issues. Let me know if you want me to:
1. Continue writing ALL issues in this detail
2. Provide a condensed summary format
3. Focus on specific epics first

---

## Sample Code (Following Sandi Metz Principles)

### Example: Cache Manager (Small Classes Pattern)

**app/cache/manager.py:**
```python
from typing import Optional
from app.cache.redis_cache import RedisCache
from app.cache.semantic_cache import SemanticCache
from app.models.cache_entry import CacheEntry
from app.models.query import QueryRequest
from app.utils.logger import get_logger

logger = get_logger(__name__)


class CacheManager:
    """
    Orchestrates cache operations.
    
    Single Responsibility: Coordinate exact and semantic caches.
    """
    
    def __init__(
        self,
        redis_cache: RedisCache,
        semantic_cache: SemanticCache
    ):
        self._redis = redis_cache
        self._semantic = semantic_cache
    
    async def fetch(
        self,
        request: QueryRequest
    ) -> Optional[CacheEntry]:
        """
        Fetch from cache using cascade strategy.
        
        1. Try exact match (Redis)
        2. Try semantic match (Qdrant)
        3. Return None if no match
        """
        if not request.use_cache:
            return None
        
        exact_match = await self._fetch_exact(request)
        if exact_match:
            return exact_match
        
        semantic_match = await self._fetch_semantic(request)
        return semantic_match
    
    async def store(self, entry: CacheEntry) -> None:
        """Store in both caches."""
        await self._store_exact(entry)
        await self._store_semantic(entry)
    
    async def _fetch_exact(
        self,
        request: QueryRequest
    ) -> Optional[CacheEntry]:
        """Fetch from Redis (exact match)."""
        try:
            entry = await self._redis.get(request.query)
            if entry:
                logger.info("cache_hit", type="exact")
            return entry
        except Exception as e:
            logger.error("redis_fetch_failed", error=str(e))
            return None
    
    async def _fetch_semantic(
        self,
        request: QueryRequest
    ) -> Optional[CacheEntry]:
        """Fetch from Qdrant (semantic match)."""
        try:
            entry = await self._semantic.search(request.query)
            if entry:
                logger.info("cache_hit", type="semantic")
            return entry
        except Exception as e:
            logger.error("semantic_fetch_failed", error=str(e))
            return None
    
    async def _store_exact(self, entry: CacheEntry) -> None:
        """Store in Redis."""
        try:
            await self._redis.set(entry)
        except Exception as e:
            logger.error("redis_store_failed", error=str(e))
    
    async def _store_semantic(self, entry: CacheEntry) -> None:
        """Store in Qdrant."""
        try:
            await self._semantic.store(entry)
        except Exception as e:
            logger.error("semantic_store_failed", error=str(e))
```

**Key Sandi Metz principles demonstrated:**
- ✅ Single Responsibility: Only orchestrates caches
- ✅ Small methods: Each method under 10 lines
- ✅ Dependency Injection: Redis and Semantic caches injected
- ✅ Clear naming: Method names describe exact action
- ✅ Error handling: Isolated in each method

---

### Example: LLM Provider Abstraction

**app/llm/provider.py:**
```python
from abc import ABC, abstractmethod
from typing import Protocol

from app.models.query import QueryRequest


class LLMResponse(Protocol):
    """LLM response protocol."""
    content: str
    prompt_tokens: int
    completion_tokens: int
    model: str


class LLMProvider(ABC):
    """
    Abstract LLM provider.
    
    Single Responsibility: Define provider interface.
    """
    
    @abstractmethod
    async def complete(self, request: QueryRequest) -> LLMResponse:
        """Generate completion."""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """Get provider name."""
        pass


class OpenAIProvider(LLMProvider):
    """OpenAI implementation."""
    
    def __init__(self, api_key: str):
        self._api_key = api_key
        self._client = None  # Lazy init
    
    async def complete(self, request: QueryRequest) -> LLMResponse:
        """Generate OpenAI completion."""
        client = self._get_client()
        response = await self._call_api(client, request)
        return self._parse_response(response)
    
    def get_name(self) -> str:
        """Return provider name."""
        return "openai"
    
    def _get_client(self):
        """Lazy client initialization."""
        if not self._client:
            from openai import AsyncOpenAI
            self._client = AsyncOpenAI(api_key=self._api_key)
        return self._client
    
    async def _call_api(self, client, request):
        """Call OpenAI API."""
        return await client.chat.completions.create(
            model=request.model or "gpt-3.5-turbo",
            messages=[{"role": "user", "content": request.query}],
            max_tokens=request.max_tokens or 1000,
            temperature=request.temperature or 0.7,
        )
    
    def _parse_response(self, response) -> LLMResponse:
        """Parse API response."""
        from app.models.llm import LLMResponseImpl
        return LLMResponseImpl(
            content=response.choices[0].message.content,
            prompt_tokens=response.usage.prompt_tokens,
            completion_tokens=response.usage.completion_tokens,
            model=response.model,
        )
```

**Key principles:**
- ✅ Abstraction: Provider interface for multiple LLMs
- ✅ Single Responsibility: Each method does ONE thing
- ✅ Dependency Injection: API key injected
- ✅ Small methods: Under 5-10 lines each
- ✅ Lazy initialization: Client created only when needed

---

## Testing Strategy

### Test Structure
```
tests/
├── unit/           # Fast, isolated, mocked
├── integration/    # Multiple components, real services
└── mocks/          # Shared mock objects
```

### Unit Test Example

**tests/unit/cache/test_cache_manager.py:**
```python
import pytest
from unittest.mock import AsyncMock, MagicMock

from app.cache.manager import CacheManager
from app.models.cache_entry import CacheEntry
from app.models.query import QueryRequest


class TestCacheManager:
    """Test CacheManager orchestration."""
    
    @pytest.fixture
    def redis_cache(self):
        """Mock Redis cache."""
        return AsyncMock()
    
    @pytest.fixture
    def semantic_cache(self):
        """Mock semantic cache."""
        return AsyncMock()
    
    @pytest.fixture
    def manager(self, redis_cache, semantic_cache):
        """Create cache manager with mocks."""
        return CacheManager(
            redis_cache=redis_cache,
            semantic_cache=semantic_cache
        )
    
    @pytest.mark.asyncio
    async def test_should_fetch_from_redis_when_exact_match(
        self,
        manager,
        redis_cache
    ):
        """Test exact cache hit from Redis."""
        # Arrange
        expected_entry = CacheEntry(
            query_hash="abc123",
            original_query="test query",
            response="test response",
            provider="openai",
            model="gpt-3.5-turbo",
            prompt_tokens=10,
            completion_tokens=20,
        )
        redis_cache.get.return_value = expected_entry
        
        request = QueryRequest(
            query="test query",
            api_key="test_key"
        )
        
        # Act
        result = await manager.fetch(request)
        
        # Assert
        assert result == expected_entry
        redis_cache.get.assert_called_once_with("test query")
    
    @pytest.mark.asyncio
    async def test_should_fetch_from_qdrant_when_no_exact_match(
        self,
        manager,
        redis_cache,
        semantic_cache
    ):
        """Test semantic cache hit from Qdrant."""
        # Arrange
        redis_cache.get.return_value = None
        expected_entry = CacheEntry(
            query_hash="def456",
            original_query="similar query",
            response="similar response",
            provider="openai",
            model="gpt-3.5-turbo",
            prompt_tokens=10,
            completion_tokens=20,
        )
        semantic_cache.search.return_value = expected_entry
        
        request = QueryRequest(
            query="test query",
            api_key="test_key"
        )
        
        # Act
        result = await manager.fetch(request)
        
        # Assert
        assert result == expected_entry
        semantic_cache.search.assert_called_once_with("test query")
    
    @pytest.mark.asyncio
    async def test_should_return_none_when_cache_disabled(
        self,
        manager,
        redis_cache,
        semantic_cache
    ):
        """Test cache bypass when disabled."""
        # Arrange
        request = QueryRequest(
            query="test query",
            use_cache=False,
            api_key="test_key"
        )
        
        # Act
        result = await manager.fetch(request)
        
        # Assert
        assert result is None
        redis_cache.get.assert_not_called()
        semantic_cache.search.assert_not_called()
```

**Test principles:**
- ✅ One assertion per test (mostly)
- ✅ Clear test names: test_should_xxx_when_yyy
- ✅ Arrange-Act-Assert structure
- ✅ Isolated with mocks
- ✅ Tests behavior, not implementation

---

This document continues for 200+ tasks. Should I:
1. Generate the complete 200+ issues now?
2. Continue with more sample code?
3. Show you the full Epic structure first?

Let me know what you'd prefer for the next section!
